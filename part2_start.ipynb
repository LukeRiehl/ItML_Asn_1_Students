{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ml_utils\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from ml_utils import edaDF\n",
    "\n",
    "##Seaborn for fancy plots. \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams[\"figure.figsize\"] = (8,8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3950 Assignment 1: Part 2\n",
    "\n",
    "For this assignment we want to use some sort of tree based model to classify the data below. We have a very small training set, so overfitting is a very real concern. \n",
    "\n",
    "Some specifics for this assignment:\n",
    "<ul>\n",
    "<li>Please use the show_eda to control if EDA stuff is shown. I don't really need to see all the EDA stuff (nor do you after you've done it), so we can make it configurable with a variable to speed up time. Please set this FALSE when you submit, so I can run all and see the outcome without histograms etc...\n",
    "<li>Please ensure that whatever model you end up with is in a variable named best at the end.\n",
    "<li>Please use some pipeline in prepping the data. The test data is in an identical format to the training data, so whatever pipeline you've created for your training will work for the testing. \n",
    "<li>The accuracy scoring will be an average of accuracy and roc_auc. \n",
    "</ul>\n",
    "\n",
    "### Grading Metrics\n",
    "<ul>\n",
    "<li><b>Pipeline Used - 10pts</b> The data loading needs to be in a pipeline. See the test part for illustration. When testing I'll call your pipe with the new data (format is identical to training), so any prep stuff should be in the pipeline. \n",
    "<li><b>Tree Based Model Used - 5pts</b> The model used for classification needs to be some variety of tree, beyond that it is up to you. \n",
    "<li><b>Accuracy - 5pts</b> The final accuracy acheived. This will be a rough ranking, I'm assuming most people will get a similar level of accuracy, marks will only be deducted if yours is far wosrse, as that's an indication that you probably didn't take any/many steps to improve things. \n",
    "<li><b>Clarity and Formatting - 5pts</b> Is it organized and can I read it?\n",
    "    <ul>\n",
    "    <li> <b>Note:</b> for this assignment, and in general, please get rid of my comments and replace them with your own. I'm going to read this, so all of these instructions aren't really required. Think of this as a template, get rid of the stuff that isn't needed, and leave only the things you need to explain your code. \n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "For submission, please drop the URL for your repository in the dropbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Lucas Riehl\"\n",
    "\n",
    "show_eda = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>...</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "      <th>var_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.968</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.795</td>\n",
       "      <td>...</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.605</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.666</td>\n",
       "      <td>0.572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  var_1  var_2  var_3  var_4  var_5  var_6  var_7  var_8  var_9  ...  \\\n",
       "0       0  0.660  0.106  0.434  0.387  0.903  0.661  0.158  0.291  0.210  ...   \n",
       "1       1  0.844  0.813  0.030  0.939  0.721  0.287  0.539  0.874  0.787  ...   \n",
       "2       0  0.560  0.567  0.568  0.434  0.414  0.180  0.448  0.888  0.023  ...   \n",
       "3       0  0.681  0.245  0.909  0.785  0.738  0.570  0.692  0.411  0.182  ...   \n",
       "4       0  0.846  0.431  0.805  0.237  0.465  0.642  0.219  0.102  0.795  ...   \n",
       "\n",
       "   var_191  var_192  var_193  var_194  var_195  var_196  var_197  var_198  \\\n",
       "0    0.015    0.377    0.479    0.050    0.395    0.123    0.833    0.461   \n",
       "1    0.112    0.048    0.088    0.860    0.560    0.346    0.511    0.883   \n",
       "2    0.874    0.236    0.599    0.602    0.005    0.493    0.122    0.395   \n",
       "3    0.219    0.691    0.261    0.031    0.968    0.353    0.798    0.104   \n",
       "4    0.704    0.242    0.089    0.605    0.577    0.043    0.686    0.070   \n",
       "\n",
       "   var_199  var_200  \n",
       "0    0.990    0.105  \n",
       "1    0.858    0.599  \n",
       "2    0.782    0.943  \n",
       "3    0.944    0.090  \n",
       "4    0.666    0.572  \n",
       "\n",
       "[5 rows x 201 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load data\n",
    "df = pd.read_csv(\"training.csv\")\n",
    "df = df.drop(columns={\"id\"})\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n"
     ]
    }
   ],
   "source": [
    "eda = ml_utils.edaDF(df,\"target\")\n",
    "print(eda.giveTarget())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_eda == True: \n",
    "    eda.fullEDA(k=1.5, scatterplot=False, optional_countplots=False, optional_histplots=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating list of columns for pipeline constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['var_1',\n",
       " 'var_2',\n",
       " 'var_3',\n",
       " 'var_4',\n",
       " 'var_5',\n",
       " 'var_6',\n",
       " 'var_7',\n",
       " 'var_8',\n",
       " 'var_9',\n",
       " 'var_10',\n",
       " 'var_11',\n",
       " 'var_12',\n",
       " 'var_13',\n",
       " 'var_14',\n",
       " 'var_15',\n",
       " 'var_16',\n",
       " 'var_17',\n",
       " 'var_18',\n",
       " 'var_19',\n",
       " 'var_20',\n",
       " 'var_21',\n",
       " 'var_22',\n",
       " 'var_23',\n",
       " 'var_24',\n",
       " 'var_25',\n",
       " 'var_26',\n",
       " 'var_27',\n",
       " 'var_28',\n",
       " 'var_29',\n",
       " 'var_30',\n",
       " 'var_31',\n",
       " 'var_32',\n",
       " 'var_33',\n",
       " 'var_34',\n",
       " 'var_35',\n",
       " 'var_36',\n",
       " 'var_37',\n",
       " 'var_38',\n",
       " 'var_39',\n",
       " 'var_40',\n",
       " 'var_41',\n",
       " 'var_42',\n",
       " 'var_43',\n",
       " 'var_44',\n",
       " 'var_45',\n",
       " 'var_46',\n",
       " 'var_47',\n",
       " 'var_48',\n",
       " 'var_49',\n",
       " 'var_50',\n",
       " 'var_51',\n",
       " 'var_52',\n",
       " 'var_53',\n",
       " 'var_54',\n",
       " 'var_55',\n",
       " 'var_56',\n",
       " 'var_57',\n",
       " 'var_58',\n",
       " 'var_59',\n",
       " 'var_60',\n",
       " 'var_61',\n",
       " 'var_62',\n",
       " 'var_63',\n",
       " 'var_64',\n",
       " 'var_65',\n",
       " 'var_66',\n",
       " 'var_67',\n",
       " 'var_68',\n",
       " 'var_69',\n",
       " 'var_70',\n",
       " 'var_71',\n",
       " 'var_72',\n",
       " 'var_73',\n",
       " 'var_74',\n",
       " 'var_75',\n",
       " 'var_76',\n",
       " 'var_77',\n",
       " 'var_78',\n",
       " 'var_79',\n",
       " 'var_80',\n",
       " 'var_81',\n",
       " 'var_82',\n",
       " 'var_83',\n",
       " 'var_84',\n",
       " 'var_85',\n",
       " 'var_86',\n",
       " 'var_87',\n",
       " 'var_88',\n",
       " 'var_89',\n",
       " 'var_90',\n",
       " 'var_91',\n",
       " 'var_92',\n",
       " 'var_93',\n",
       " 'var_94',\n",
       " 'var_95',\n",
       " 'var_96',\n",
       " 'var_97',\n",
       " 'var_98',\n",
       " 'var_99',\n",
       " 'var_100',\n",
       " 'var_101',\n",
       " 'var_102',\n",
       " 'var_103',\n",
       " 'var_104',\n",
       " 'var_105',\n",
       " 'var_106',\n",
       " 'var_107',\n",
       " 'var_108',\n",
       " 'var_109',\n",
       " 'var_110',\n",
       " 'var_111',\n",
       " 'var_112',\n",
       " 'var_113',\n",
       " 'var_114',\n",
       " 'var_115',\n",
       " 'var_116',\n",
       " 'var_117',\n",
       " 'var_118',\n",
       " 'var_119',\n",
       " 'var_120',\n",
       " 'var_121',\n",
       " 'var_122',\n",
       " 'var_123',\n",
       " 'var_124',\n",
       " 'var_125',\n",
       " 'var_126',\n",
       " 'var_127',\n",
       " 'var_128',\n",
       " 'var_129',\n",
       " 'var_130',\n",
       " 'var_131',\n",
       " 'var_132',\n",
       " 'var_133',\n",
       " 'var_134',\n",
       " 'var_135',\n",
       " 'var_136',\n",
       " 'var_137',\n",
       " 'var_138',\n",
       " 'var_139',\n",
       " 'var_140',\n",
       " 'var_141',\n",
       " 'var_142',\n",
       " 'var_143',\n",
       " 'var_144',\n",
       " 'var_145',\n",
       " 'var_146',\n",
       " 'var_147',\n",
       " 'var_148',\n",
       " 'var_149',\n",
       " 'var_150',\n",
       " 'var_151',\n",
       " 'var_152',\n",
       " 'var_153',\n",
       " 'var_154',\n",
       " 'var_155',\n",
       " 'var_156',\n",
       " 'var_157',\n",
       " 'var_158',\n",
       " 'var_159',\n",
       " 'var_160',\n",
       " 'var_161',\n",
       " 'var_162',\n",
       " 'var_163',\n",
       " 'var_164',\n",
       " 'var_165',\n",
       " 'var_166',\n",
       " 'var_167',\n",
       " 'var_168',\n",
       " 'var_169',\n",
       " 'var_170',\n",
       " 'var_171',\n",
       " 'var_172',\n",
       " 'var_173',\n",
       " 'var_174',\n",
       " 'var_175',\n",
       " 'var_176',\n",
       " 'var_177',\n",
       " 'var_178',\n",
       " 'var_179',\n",
       " 'var_180',\n",
       " 'var_181',\n",
       " 'var_182',\n",
       " 'var_183',\n",
       " 'var_184',\n",
       " 'var_185',\n",
       " 'var_186',\n",
       " 'var_187',\n",
       " 'var_188',\n",
       " 'var_189',\n",
       " 'var_190',\n",
       " 'var_191',\n",
       " 'var_192',\n",
       " 'var_193',\n",
       " 'var_194',\n",
       " 'var_195',\n",
       " 'var_196',\n",
       " 'var_197',\n",
       " 'var_198',\n",
       " 'var_199',\n",
       " 'var_200']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eda.num"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridsearchCV so I can find the best model and hyperparameters to fit my model. Train test and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(max_depth=20, min_samples_leaf=4,\n",
      "                                        min_samples_split=5))])\n",
      "0.5396825396825397\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import column_or_1d\n",
    "\n",
    "y = df[\"target\"]\n",
    "X = df.drop(columns={\"target\"})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "y_test=y_test.ravel()\n",
    "y_train=y_train.ravel()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "# Define the parameters for the grid search\n",
    "param_grid = {\n",
    "    # 'classifier__n_estimators': [10, 50, 100],\n",
    "    'classifier__max_depth': [5, 10, 20],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(grid_search.best_estimator_)\n",
    "\n",
    "# Print the accuracy on the test set\n",
    "print(grid_search.score(X_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline constructor, only numeric was used cause we have all numeric data. Then i plugged that into my preprocessor to be used in my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numeric_transformer = Pipeline( steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "preprocessor = Pipeline( steps=[\n",
    "        (\"num\", numeric_transformer)\n",
    "    ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implemented my tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.893048128342246\n",
      "Testing Accuracy: 0.6507936507936508\n",
      "DecisionTreeClassifier(max_depth=10, max_features=7, min_samples_leaf=4,\n",
      "                       random_state=42)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best=Pipeline(steps=[('pre', preprocessor), ('classifier', DecisionTreeClassifier(max_depth=10, min_samples_leaf=4, max_features=7, random_state=42))])\n",
    "\n",
    "best.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training Accuracy:\", best.score(X_train, y_train))\n",
    "print(\"Testing Accuracy:\", best.score(X_test, y_test)) \n",
    "print(best._final_estimator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Everything below was just me testing out other models and trying to see what worked and what didnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Define the pipeline with a StandardScaler and a RandomForestClassifier\n",
    "# pipeline = Pipeline([\n",
    "#     ('scaler', StandardScaler()),\n",
    "#     ('classifier', RandomForestClassifier(n_estimators=100, max_depth=20, min_samples_split=5, min_samples_leaf=1))\n",
    "# ])\n",
    "\n",
    "# # Fit the pipeline to the training data\n",
    "# pipeline.fit(X_train, y_train)\n",
    "\n",
    "# # Print the train accuracy\n",
    "# print(\"Train accuracy:\", pipeline.score(X_train, y_train))\n",
    "\n",
    "# # Print the test accuracy\n",
    "# print(\"Test accuracy:\", pipeline.score(X_test, y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Define the pipeline with a StandardScaler and a RandomForestClassifier\n",
    "# pipeline = Pipeline([\n",
    "#     ('scaler', StandardScaler()),\n",
    "#     ('classifier', RandomForestClassifier(n_estimators=100, max_depth=20, min_samples_split=5, min_samples_leaf=1, \n",
    "#                                           max_features='sqrt', random_state=42, criterion='entropy', \n",
    "#                                           min_impurity_decrease=0.01))\n",
    "# ])\n",
    "\n",
    "# # Fit the pipeline to the training data\n",
    "# pipeline.fit(X_train, y_train)\n",
    "\n",
    "# # Print the train accuracy\n",
    "# print(\"Train accuracy:\", pipeline.score(X_train, y_train))\n",
    "\n",
    "# # Print the test accuracy\n",
    "# print(\"Test accuracy:\", pipeline.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Define the pipeline with a StandardScaler and an ExtraTreesClassifier\n",
    "# pipeline = Pipeline([\n",
    "#     ('scaler', StandardScaler()),\n",
    "#     ('classifier', ExtraTreesClassifier(n_estimators=100, max_depth=10, min_samples_split=2, min_samples_leaf=1, \n",
    "#                                           max_features='sqrt', random_state=42, criterion='entropy', \n",
    "#                                           min_impurity_decrease=0.01, bootstrap=True, n_jobs=-1))\n",
    "# ])\n",
    "\n",
    "# # Fit the pipeline to the training data\n",
    "# pipeline.fit(X_train, y_train)\n",
    "\n",
    "# # Print the train accuracy\n",
    "# print(\"Train accuracy:\", pipeline.score(X_train, y_train))\n",
    "\n",
    "# # Print the test accuracy\n",
    "# print(\"Test accuracy:\", pipeline.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import BaggingClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Define the pipeline with a StandardScaler and a BaggingClassifier\n",
    "# pipeline = Pipeline([\n",
    "#     ('scaler', StandardScaler()),\n",
    "#     ('classifier', BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=42), n_estimators=100, max_samples=0.5, max_features=1.0, bootstrap=True, bootstrap_features=False, n_jobs=-1))\n",
    "# ])\n",
    "\n",
    "# # Fit the pipeline to the training data\n",
    "# pipeline.fit(X_train, y_train)\n",
    "\n",
    "# # Print the train accuracy\n",
    "# print(\"Train accuracy:\", pipeline.score(X_train, y_train))\n",
    "\n",
    "# # Print the test accuracy\n",
    "# print(\"Test accuracy:\", pipeline.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6507936507936508\n",
      "Pipeline(steps=[('pre',\n",
      "                 Pipeline(steps=[('num',\n",
      "                                  Pipeline(steps=[('imputer', SimpleImputer()),\n",
      "                                                  ('scaler',\n",
      "                                                   StandardScaler())]))])),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(max_depth=10, max_features=7,\n",
      "                                        min_samples_leaf=4, random_state=42))])\n"
     ]
    }
   ],
   "source": [
    "print(best.score(X_test, y_test))\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "Please leave the stuff below as-is in your file. \n",
    "\n",
    "This will take your best model and score it with the test data. If you want to test to make sure that yours works, make a copy of the data file and rename it testing.csv, then make sure this runs ok. I will do the same, but the contents of my test file will be different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'testing.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Luke\\ItML_Asn_1_Students\\part2_start.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Luke/ItML_Asn_1_Students/part2_start.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#Load Test Data\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Luke/ItML_Asn_1_Students/part2_start.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m test_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39mtesting.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Luke/ItML_Asn_1_Students/part2_start.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m test_df \u001b[39m=\u001b[39m test_df\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m})\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Luke/ItML_Asn_1_Students/part2_start.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#Create tests and score\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Luke\\anaconda3\\envs\\backup_env\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Luke\\anaconda3\\envs\\backup_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\Luke\\anaconda3\\envs\\backup_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Luke\\anaconda3\\envs\\backup_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\Luke\\anaconda3\\envs\\backup_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m     f,\n\u001b[0;32m   1219\u001b[0m     mode,\n\u001b[0;32m   1220\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1221\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1222\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1223\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1224\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1225\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1226\u001b[0m )\n\u001b[0;32m   1227\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Luke\\anaconda3\\envs\\backup_env\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    790\u001b[0m             handle,\n\u001b[0;32m    791\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    792\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    793\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    794\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    795\u001b[0m         )\n\u001b[0;32m    796\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'testing.csv'"
     ]
    }
   ],
   "source": [
    "#Load Test Data\n",
    "test_df = pd.read_csv(\"testing.csv\")\n",
    "test_df = test_df.drop(columns={\"id\"})\n",
    "#Create tests and score\n",
    "test_y = np.array(test_df[\"target\"]).reshape(-1,1)\n",
    "test_X = np.array(test_df.drop(columns={\"target\"}))\n",
    "\n",
    "preds = best.predict(test_X)\n",
    "\n",
    "roc_score = roc_auc_score(test_y, preds)\n",
    "acc_score = accuracy_score(test_y, preds)\n",
    "\n",
    "print(roc_score)\n",
    "print(acc_score)\n",
    "print(name, np.mean([roc_score, acc_score]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Accuracy Changes Were Used\n",
    "\n",
    "Please list here what you did to try to increase accuracy and/or limit overfitting:\n",
    "<li>Used GridsearchCV to help me find the most accurate model to use and the most accurate hyperparameters\n",
    "<li>used max depth and experimented the amount of layers to balance overfitting\n",
    "<li>min_samples_leaf was used to control the minimum number of samples required to be at a leaf node, this helped to prevent overfitting by making the tree more robust to noise in the data.\n",
    "<li> And lastly I just expereimented with different tree models and tried to get to using different hyperparamters. This led to me fine tuning what worked best and then finally giving me a reasonably accurate score while compensating for overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backup_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9609b5c280f9bc4c749d9d85ef94fb536c220c07358446be80324bb378b29e92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
